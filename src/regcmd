liveplot = false
enableCuda = false
ClassNLL = true

model:forward(test_data[1].data)
model:forward(test_data[2].data)
model:forward(test_data[3].data)
model:forward(test_data[4].data)

dofile "readImage.lua"
i=1
input = train_data[i].data
target = train_data[i].labels
output = model:forward(input)
--err = criterion:forward(output, target)
target
output
--err
--------------------------------------------------
-- two different model and want to flattenParameters
-- in this example flattenParameters failed
--
require 'cunn'
require 'cutorch'
require 'torch'
require 'nn'
dofile 'utils.lua'

criterion = nn.MSECriterion()
inputs = torch.ones(6)
target = torch.ones(1)*10
TreeModels = {nn.Sequential(),nn.Sequential()}
TreeModels[1]:add(nn.Linear(6,1))
TreeModels[2]:add(nn.Linear(6,1))
parameters1,gradParameters1 = TreeModels[1]:getParameters()
parameters2,gradParameters2 = TreeModels[2]:getParameters()

parameters = {}
gradParameters = {}
parameters = {parameters1,parameters2}
gradParameters = {gradParameters1,gradParameters2,gradParameters3,gradParameters4}
parametersList = flattenParameters(parameters)
gradParametersList = flattenParameters(gradParameters)
gradParametersList:zero()

gradParameters1:zero()
output = TreeModels[1]:forward(inputs)
err = criterion:forward(output, target)
dfdx = criterion:backward(output, target)
TreeModels[1]:backward(inputs, dfdx)
parameters1,gradParameters1
TreeModels[1]:getParameters()

----------------------------------------------
-- two different model
-- in this example each gradParameters[x] update
--
require 'torch'
require 'nn'
dofile 'utils.lua'
criterion = nn.MSECriterion()
input = torch.ones(4)
target = torch.ones(1) * 10
model = {nn.Sequential(),nn.Sequential()}
model[1]:add(nn.Linear(4, 1))
model[2]:add(nn.Linear(4, 1))
parameters = {}
gradParameters = {}
parameters[1],gradParameters[1] = model[1]:getParameters()
parameters[2],gradParameters[2] = model[2]:getParameters()
bla = gradParameters[1]:zero()
bla = gradParameters[2]:zero()
--parametersList = flattenParameters(parameters)
--gradParametersList = flattenParameters(gradParameters)
--bla = gradParametersList:zero()
branch = 2
output = model[branch]:forward(input)
err = criterion:forward(output, target)
df_do = criterion:backward(output, target)
print("print(gradParametersList):")
print(gradParameters[branch])
bla = model[branch]:backward(input, df_do)
print(gradParameters[branch])

-----------------------------------------------
-- with Node, single root
-- in this example each gradParameters[x] update
--
-- this pattern will be used to build my Tree CNN
--
require 'torch'
require 'nn'

criterion = nn.MSECriterion()
input = torch.ones(4)
target = torch.ones(1) * 10
modelNode = {}
modelNode[1] = {nn.Sequential()}
modelNode[2] = {nn.Sequential(),nn.Sequential()}
modelNode[1][1]:add(nn.Linear(4, 2))
modelNode[2][1]:add(nn.Linear(2, 1))
modelNode[2][2] = modelNode[2][1]:clone()
TreeModels = {nn.Sequential(),nn.Sequential()}
function modelGenerater(branchNum)
    TreeModels[branchNum]:add(modelNode[1][1])
    TreeModels[branchNum]:add(modelNode[2][branchNum])
end
for i = 1,2 do 
    modelGenerater(i)
end
parameters = {}
gradParameters = {}
parameters[1],gradParameters[1] = modelNode[2][1]:getParameters()
parameters[2],gradParameters[2] = modelNode[2][2]:getParameters()
parameters[3],gradParameters[3] = modelNode[1][1]:getParameters()
bla = gradParameters[1]:zero()
bla = gradParameters[2]:zero()
bla = gradParameters[3]:zero()

branch = 1
output = TreeModels[branch]:forward(input)
err = criterion:forward(output, target)
df_do = criterion:backward(output, target)
print(gradParameters[3])
bla = TreeModels[branch]:backward(input, df_do)
print(gradParameters[3])

--------------------------------------------------
-- without Node, single root
-- in this example each gradParameters[x] update
--
require 'torch'
require 'nn'
criterion = nn.MSECriterion()
input = torch.ones(4)
target = torch.ones(1) * 10
root = nn.Linear(4, 2)
leaf = {}
leaf[1] = nn.Linear(2, 1)
leaf[2] = nn.Linear(2, 1)
TreeModels = {nn.Sequential(),nn.Sequential()}
function modelGenerater(branchNum)
    TreeModels[branchNum]:add(root)
    TreeModels[branchNum]:add(leaf[branchNum])
end
for i = 1,2 do 
    modelGenerater(i)
end
parameters = {}
gradParameters = {}
parameters[1],gradParameters[1] = leaf[1]:getParameters()
parameters[2],gradParameters[2] = leaf[2]:getParameters()
parameters[3],gradParameters[3] = root:getParameters()
bla = gradParameters[1]:zero()
bla = gradParameters[2]:zero()
bla = gradParameters[3]:zero()

branch = 2
output = TreeModels[branch]:forward(input)
err = criterion:forward(output, target)
df_do = criterion:backward(output, target)
print(gradParameters[3])
bla = TreeModels[branch]:backward(input, df_do)
print(gradParameters[3])

-----------------------------------------------
-- with Node, mulit-root shared
-- in this example each gradParameters[x] update
--
require 'torch'
require 'nn'

criterion = nn.MSECriterion()
input = torch.ones(4)
target = torch.ones(1) * 10
modelNode = {}
modelNode[1] = {nn.Sequential(),nn.Sequential()}
modelNode[2] = {nn.Sequential(),nn.Sequential()}
modelNode[1][1]:add(nn.Linear(4, 2))
modelNode[1][2]:add(nn.Linear(4, 2))
modelNode[1][2]:share(modelNode[1][1],'weight','bias');
modelNode[2][1]:add(nn.Linear(2, 1))
modelNode[2][2]:add(nn.Linear(2, 1))
TreeModels = {nn.Sequential(),nn.Sequential()}
function modelGenerater(branchNum)
    TreeModels[branchNum]:add(modelNode[1][branchNum])
    TreeModels[branchNum]:add(modelNode[2][branchNum])
end
for i = 1,2 do 
    modelGenerater(i)
end
parameters = {}
gradParameters = {}
parameters[1],gradParameters[1] = TreeModels[1]:getParameters()
parameters[2],gradParameters[2] = TreeModels[2]:getParameters()
bla = gradParameters[1]:zero()
bla = gradParameters[2]:zero()

branch = 2
output = TreeModels[branch]:forward(input)
err = criterion:forward(output, target)
df_do = criterion:backward(output, target)
print(gradParameters[branch])
bla = TreeModels[branch]:backward(input, df_do)
print(gradParameters[branch])

------------------------------------------------------------
-- with Node, mulit-root shared
-- in this example each parameters[x] shared
--
require 'torch'
require 'nn'
modelNode = {}
modelNode[1] = {nn.Sequential(),nn.Sequential()}
modelNode[2] = {nn.Sequential(),nn.Sequential()}
modelNode[1][1]:add(nn.Linear(4, 2));
modelNode[1][2]:add(nn.Linear(4, 2));
modelNode[1][2]:share(modelNode[1][1],'weight','bias');
modelNode[2][1]:add(nn.Linear(2, 1))
modelNode[2][2]:add(nn.Linear(2, 1))
TreeModels = {nn.Sequential(),nn.Sequential()}
function modelGenerater(branchNum)
    TreeModels[branchNum]:add(modelNode[1][branchNum])
    TreeModels[branchNum]:add(modelNode[2][branchNum])
end
for i = 1,2 do 
    modelGenerater(i)
end

modelNode[1][1]:get(1).bias:fill(1);
modelNode[1][1]:get(1).bias
modelNode[1][2]:get(1).bias

------------------------------------------------------------
-- with Node, mulit-root shared
-- in this example each parameters[x] not shared
--
require 'torch'
require 'nn'
modelNode = {}
modelNode[1] = {nn.Sequential(),nn.Sequential()}
modelNode[2] = {nn.Sequential(),nn.Sequential()}
modelNode[1][1]:add(nn.Linear(4, 2));
modelNode[1][2]:add(nn.Linear(4, 2));
modelNode[1][2]:share(modelNode[1][1],'weight','bias');
modelNode[2][1]:add(nn.Linear(2, 1))
modelNode[2][2]:add(nn.Linear(2, 1))
TreeModels = {nn.Sequential(),nn.Sequential()}
function modelGenerater(branchNum)
    TreeModels[branchNum]:add(modelNode[1][branchNum])
    TreeModels[branchNum]:add(modelNode[2][branchNum])
end
for i = 1,2 do 
    modelGenerater(i)
end
parameters = {}
gradParameters = {}
parameters[1],gradParameters[1] = TreeModels[1]:getParameters()
parameters[2],gradParameters[2] = TreeModels[2]:getParameters()
modelNode[1][2]:share(modelNode[1][1],'weight','bias');
modelNode[1][1]:get(1).bias:fill(1);
modelNode[1][1]:get(1).bias
modelNode[1][2]:get(1).bias

-------------------------------------------------------
require 'torch'
require 'nn'

criterion = nn.MSECriterion()
inputs = torch.ones(4)*5
i = -1
inputs:apply(function()
  i = i + 1
  return i
end)

target = torch.ones(1)*10
model=(nn.Linear(4,1))

b:zero()
output = model:forward(inputs)
err = criterion:forward(output, target)
dfdx = criterion:backward(output, target)
output
err
dfdx
model:backward(inputs, dfdx)
a,b = model:getParameters()
